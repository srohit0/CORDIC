{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ABCs Of PyTorch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srohit0/CORDIC/blob/master/ABCs_Of_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztuQAkroR9Sr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hQjCUITR_VW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tensors\n",
        "x = torch.FloatTensor([[1,2,3], [4,5,6]])\n",
        "print(x.size(), \"\\n\", x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZ0gFjpcTztT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add tensors\n",
        "x.add_(torch.ones([2,3])+torch.ones([2,3]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2qm-KsDUCMp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Subtract Tensor\n",
        "x.sub_(torch.ones([2,3])*2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h08RVzO9VsOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Numpy to torch tensors\n",
        "import numpy as np\n",
        "\n",
        "y = np.matrix([[2,2],[2,2],[2,2]])\n",
        "z = np.matrix([[2,2],[2,2],[2,2]], dtype='int16')\n",
        "x.short() @ torch.from_numpy(z)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4I8_oJ5XtiP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reshape tensors (similar to np.reshape)\n",
        "x.view(1,6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BleWb7rIZTF-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# move variables and copies across compute devices.\n",
        "\n",
        "x = torch.FloatTensor([[1,2,3], [4,5,6]])\n",
        "y = np.matrix([[2,2,2],[2,2,2]], dtype='float32')\n",
        "\n",
        "if ( torch.cuda.is_available() ):\n",
        "  x = x.cuda();\n",
        "  y = torch.from_numpy(y).cuda()\n",
        "  z = x+y\n",
        "print(z)\n",
        "\n",
        "print(x.cpu())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcdN6mIobTYo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Variable (part of autograd package)\n",
        "# Variables (graph nodes) are thin wrappers around tensors and have dependency knowledge\n",
        "# Variables enable backpropagation of gradients and automatic differentiations\n",
        "# Variable are set a 'volatile' flad during infrencing.\n",
        "\n",
        "from torch.autograd import Variable\n",
        "v1 = Variable(torch.tensor([1.,2.,3.]), requires_grad=False)\n",
        "v2 = Variable(torch.tensor([4.,5.,6.]), requires_grad=True)\n",
        "v3 = v1 * v2 \n",
        "\n",
        "v3.data.numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TE6sxJOjwcH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Variables remember what created them.\n",
        "v3.grad_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYZEQmBMxkBl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Backpropagation with example of sin(x)\n",
        "x = Variable(torch.Tensor(np.array([0., 1., 1.5, 2.])*np.pi), requires_grad=True)\n",
        "y = torch.sin(x)\n",
        "x.grad\n",
        "y.backward(torch.Tensor([1.,1.,1.,1]))\n",
        "\n",
        "# Check gradient is indeed cox(x)\n",
        "if ( (x.grad.data.int().numpy() == torch.cos(x).data.int().numpy()).all() ):\n",
        "  print (\"d(sin(x)/dx = cos(x))\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CG3bjfV6yc46",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Simple Liner Regression\n",
        "#         Fit a line to the data. Y = w.x + b\n",
        "\n",
        "# Deterministic behavior\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "\n",
        "# Step 1: Dataset\n",
        "\n",
        "w = 2; b = 3\n",
        "x = np.linspace(0, 10, 100)\n",
        "y = w*x + b + np.random.randn(100)*2\n",
        "x = x.reshape(-1, 1)\n",
        "y = y.reshape(-1, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIMaspKb37MO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 2: Model\n",
        "\n",
        "class LinearRegressionModel(torch.nn.Module):\n",
        "  \n",
        "  def __init__(self, in_dimn, out_dimn):\n",
        "    super(LinearRegressionModel, self).__init__()\n",
        "    self.model = torch.nn.Linear(in_dimn, out_dimn)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    y_pred = self.model(x);\n",
        "    return y_pred;\n",
        "\n",
        "model = LinearRegressionModel(in_dimn=1, out_dimn=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qh2jbiM05yA6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 3: Training\n",
        "\n",
        "cost      = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "inputs    = Variable(torch.from_numpy(x.astype('float32')))\n",
        "outputs   = Variable(torch.from_numpy(y.astype('float32')))\n",
        "\n",
        "for epoch in range(100):\n",
        "  # 3.1 forward pass:\n",
        "  y_pred = model(inputs)\n",
        "  \n",
        "  # 3.2 compute loss\n",
        "  loss = cost(y_pred, outputs)\n",
        "  \n",
        "  # 3.3 backward pass\n",
        "  optimizer.zero_grad(); # by default, gradient accumulate\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  if ( (epoch+1) % 10 == 0 ):\n",
        "    print('epoch {}, loss {}'.format(epoch+1, loss.data)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZ-R5WZo_btV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 4: Display model and confirm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.title('Model and Dataset')\n",
        "plt.xlabel('X'); plt.ylabel('Y')\n",
        "plt.grid()\n",
        "plt.plot(x, y, 'ro', label='Dataset', marker='x', markersize=4)\n",
        "plt.plot(x, model.model.weight.item()*x+model.model.bias.item(), label='Regression Model')\n",
        "plt.legend(); plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}